{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DQN_tensorlayer.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h5zCvRT6ZJvC"
      },
      "source": [
        "实验平台：Google Colab\n",
        "算法：DQN Nature 版\n",
        "主要依赖包：TensorFlow2.0、TensorLayer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p2T_PAVVGAB7"
      },
      "source": [
        "import argparse\n",
        "import os\n",
        "import time\n",
        "import random"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kVPYwA0RGezm"
      },
      "source": [
        "!pip install tensorlayer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i0O6E9lDVi2r"
      },
      "source": [
        "!pip uninstall tensorflow\n",
        "!pip install tensorflow==2.0\n",
        "!pip install tensorflow-gpu==2.0   #自行指定tf版本"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NIzK3CEgW54l"
      },
      "source": [
        "import os\n",
        "os.kill(os.getpid(), 9)        # 安装完指定版本tf后重启"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "27jRcL5EGeFY"
      },
      "source": [
        "import gym\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorlayer as tl"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "yGaV6l5rV3Py",
        "outputId": "6c5c9755-c25f-447a-daf6-cdc767d31a52"
      },
      "source": [
        "tf.__version__"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2.0.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rkh4RG53Ge2h",
        "outputId": "b028744e-cba9-467c-c160-210011c4a6e2"
      },
      "source": [
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('--train', dest='train', default=True)\n",
        "parser.add_argument('--test', dest='test', default=False)\n",
        "\n",
        "parser.add_argument('--gamma', type=float, default=0.95)\n",
        "parser.add_argument('--lr', type=float, default=0.005)\n",
        "parser.add_argument('--batch_size', type=int, default=32)\n",
        "parser.add_argument('--eps', type=float, default=0.1)\n",
        "\n",
        "parser.add_argument('--train_episodes', type=int, default=200)\n",
        "parser.add_argument('--test_episodes', type=int, default=10)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "_StoreAction(option_strings=['--test_episodes'], dest='test_episodes', nargs=None, const=None, default=10, type=<class 'int'>, choices=None, help=None, metavar=None)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T5XzdyQDGe5v"
      },
      "source": [
        "# 解决 SystemExit: 2 错误\n",
        "import sys\n",
        "sys.argv=['']\n",
        "del sys"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_vEcmnhTGe8V"
      },
      "source": [
        "args = parser.parse_args()"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YM8zuiIiGfLw"
      },
      "source": [
        "ALG_NAME = 'DQN'\n",
        "ENV_ID = 'CartPole-v1'"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ftwFySQpK6vC"
      },
      "source": [
        "class ReplayBuffer:\n",
        "  def __init__(self, capacity=10000):\n",
        "    self.capacity = capacity\n",
        "    self.buffer = []\n",
        "    self.position = 0\n",
        "  \n",
        "  def push(self, state, action, reward, next_state, done):\n",
        "    if len(self.buffer) < self.capacity:\n",
        "      self.buffer.append(None)\n",
        "    self.buffer[self.position] = (state, action, reward, next_state, done)\n",
        "    self.position = int((self.position + 1) % self.capacity)\n",
        "  \n",
        "  def sample(self, batch_size=args.batch_size):\n",
        "    batch = random.sample(self.buffer, batch_size)\n",
        "    state, action, reward, next_state, done = map(np.stack, zip(*batch))\n",
        "    return state, action, reward, next_state, done"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4_Fy-UsYK61A"
      },
      "source": [
        "class Agent:\n",
        "    def __init__(self, env):\n",
        "        self.env = env\n",
        "        self.state_dim = self.env.observation_space.shape[0]\n",
        "        self.action_dim = self.env.action_space.n\n",
        "\n",
        "        def create_model(input_state_shape):\n",
        "            input_layer = tl.layers.Input(input_state_shape)\n",
        "            layer_1 = tl.layers.Dense(n_units=32, act=tf.nn.relu)(input_layer)\n",
        "            layer_2 = tl.layers.Dense(n_units=16, act=tf.nn.relu)(layer_1)\n",
        "            output_layer = tl.layers.Dense(n_units=self.action_dim)(layer_2)\n",
        "            return tl.models.Model(inputs=input_layer, outputs=output_layer)\n",
        "\n",
        "        self.model = create_model([None, self.state_dim])\n",
        "        self.target_model = create_model([None, self.state_dim])\n",
        "        self.model.train()\n",
        "        self.target_model.eval()\n",
        "        self.model_optim = self.target_model_optim = tf.optimizers.Adam(lr=args.lr)\n",
        "\n",
        "        self.epsilon = args.eps\n",
        "\n",
        "        self.buffer = ReplayBuffer()\n",
        "\n",
        "    def target_update(self):\n",
        "        \"\"\"Copy q network to target q network\"\"\"\n",
        "        for weights, target_weights in zip(\n",
        "                self.model.trainable_weights, self.target_model.trainable_weights):\n",
        "            target_weights.assign(weights)\n",
        "\n",
        "    def choose_action(self, state):\n",
        "        if np.random.uniform() < self.epsilon:\n",
        "            return np.random.choice(self.action_dim)\n",
        "        else:\n",
        "            q_value = self.model(state[np.newaxis, :])[0]\n",
        "            return np.argmax(q_value)\n",
        "\n",
        "    def replay(self):\n",
        "        for _ in range(10):\n",
        "            # sample an experience tuple from the dataset(buffer)\n",
        "            states, actions, rewards, next_states, done = self.buffer.sample()\n",
        "            # compute the target value for the sample tuple\n",
        "            # targets [batch_size, action_dim]\n",
        "            # Target represents the current fitting level\n",
        "            target = self.target_model(states).numpy()\n",
        "            # next_q_values [batch_size, action_dim]\n",
        "            next_target = self.target_model(next_states)\n",
        "            next_q_value = tf.reduce_max(next_target, axis=1)\n",
        "            target[range(args.batch_size), actions] = rewards + (1 - done) * args.gamma * next_q_value\n",
        "\n",
        "            # use sgd to update the network weight\n",
        "            with tf.GradientTape() as tape:\n",
        "                q_pred = self.model(states)\n",
        "                loss = tf.losses.mean_squared_error(target, q_pred)\n",
        "            grads = tape.gradient(loss, self.model.trainable_weights)\n",
        "            self.model_optim.apply_gradients(zip(grads, self.model.trainable_weights))\n",
        "\n",
        "\n",
        "    def test_episode(self, test_episodes):\n",
        "        for episode in range(test_episodes):\n",
        "            state = self.env.reset().astype(np.float32)\n",
        "            total_reward, done = 0, False\n",
        "            while not done:\n",
        "                action = self.model(np.array([state], dtype=np.float32))[0]\n",
        "                action = np.argmax(action)\n",
        "                next_state, reward, done, _ = self.env.step(action)\n",
        "                next_state = next_state.astype(np.float32)\n",
        "\n",
        "                total_reward += reward\n",
        "                state = next_state\n",
        "                self.env.render()\n",
        "            print(\"Test {} | episode rewards is {}\".format(episode, total_reward))\n",
        "\n",
        "    def train(self, train_episodes=200):\n",
        "        if args.train:\n",
        "            for episode in range(train_episodes):\n",
        "                total_reward, done = 0, False\n",
        "                state = self.env.reset().astype(np.float32)\n",
        "                while not done:\n",
        "                    action = self.choose_action(state)\n",
        "                    next_state, reward, done, _ = self.env.step(action)\n",
        "                    next_state = next_state.astype(np.float32)\n",
        "                    self.buffer.push(state, action, reward, next_state, done)\n",
        "                    total_reward += reward\n",
        "                    state = next_state\n",
        "                    # self.render()\n",
        "                if len(self.buffer.buffer) > args.batch_size:\n",
        "                    self.replay()\n",
        "                    self.target_update()\n",
        "                print('EP{} EpisodeReward={}'.format(episode, total_reward))\n",
        "                # if episode % 10 == 0:\n",
        "                #     self.test_episode()\n",
        "            self.saveModel()\n",
        "        if args.test:\n",
        "            self.loadModel()\n",
        "            self.test_episode(test_episodes=args.test_episodes)\n",
        "\n",
        "    def saveModel(self):\n",
        "        path = os.path.join('model', '_'.join([ALG_NAME, ENV_ID]))\n",
        "        if not os.path.exists(path):\n",
        "            os.makedirs(path)\n",
        "        tl.files.save_weights_to_hdf5(os.path.join(path, 'model.hdf5'), self.model)\n",
        "        tl.files.save_weights_to_hdf5(os.path.join(path, 'target_model.hdf5'), self.target_model)\n",
        "        print('Saved weights.')\n",
        "\n",
        "    def loadModel(self):\n",
        "        path = os.path.join('model', '_'.join([ALG_NAME, ENV_ID]))\n",
        "        if os.path.exists(path):\n",
        "            print('Load DQN Network parametets ...')\n",
        "            tl.files.load_hdf5_to_weights_in_order(os.path.join(path, 'model.hdf5'), self.model)\n",
        "            tl.files.load_hdf5_to_weights_in_order(os.path.join(path, 'target_model.hdf5'), self.target_model)\n",
        "            print('Load weights!')\n",
        "        else: print(\"No model file find, please train model first...\")"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5G6WN-eOK62y"
      },
      "source": [
        "if __name__ == '__main__':\n",
        "    env = gym.make(ENV_ID)\n",
        "    agent = Agent(env)\n",
        "    agent.train(train_episodes=args.train_episodes)\n",
        "    env.close()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}